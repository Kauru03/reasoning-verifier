**Reasoning Verifier: A Teacherâ€“Solverâ€“Verifier Framework**

This repository provides a modular, "Resume-Safe" pipeline for training and evaluating a Smaller Model (Verifier) using reasoning generated by a Larger Model (Teacher). The goal is to develop a verifier capable of scoring the logical steps of a solver model to improve overall reasoning accuracy.

ðŸ“‚ Repository Structure & Module Descriptions
Each component is decoupled to ensure the codebase is easy to navigate and maintain for InnerSource contributors.

1. Root Files
main.py: The central entry point. It orchestrates the 6-step pipeline and handles command-line arguments.
requirements.txt: Lists all necessary Python libraries with specific version constraints for reproducibility.
.env: (Excluded via .gitignore) Stores sensitive Azure OpenAI credentials.
.gitignore: Prevents secrets, cache files, and large model weights from being tracked by Git.

2. Core Modules (src/)
larger_model.py: 
Functions: generate_response() - Description: Manages interactions with high-capacity models (e.g., GPT-4) via Azure OpenAI. It provides the "gold-standard" reasoning used for training.
smaller_model.py:
Functions: train_verifier(), score_outputs() - Description: The engine for the verifier. It handles LoRA (Low-Rank Adaptation) fine-tuning of the base model and uses the resulting weights to score solver outputs.
evaluator.py: 
Functions: calculate_gating_metrics(), compute_reasoning_similarity() - Description: Provides scientific validation. It calculates the accuracy of the verifier's "gating" (its ability to distinguish correct from incorrect answers) and the semantic similarity between model reasonings.
data_loader.py:
Functions: load_jsonl(), save_jsonl(), file_exists() - Description: Standardizes data I/O and implements the Resume-Safe logic, allowing the pipeline to skip already completed steps if the program is restarted.
config.py & logger.py:
Description: Utility files that centralize environment variables and professional logging, replacing messy print statements with timestamped logs.


ðŸš€ Step-by-Step Setup and Execution
1. Prerequisites
Ensure you have Python 3.9+ installed. It is highly recommended to use a virtual environment:
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

2. Configure Credentials
Create a file named .env in the root directory and add your Azure OpenAI details:
AZURE_OPENAI_API_KEY=your_api_key_here
AZURE_OPENAI_ENDPOINT=your_endpoint_url_here
AZURE_VERSION=2024-02-15-preview
LARGER_MODEL_NAME=gpt-4.1
SMALLER_MODEL_BASE=Qwen/Qwen2.5-3B-Instruct

3. Run the Pipeline
Execute the full process by providing your training and test datasets:
python main.py \
    --train_data data/train.jsonl \
    --test_data data/test.jsonl \
    --threshold 0.8 \
    --max_train 100 \
    --max_test 50

ðŸ”„ The Research Pipeline Explained
Teacher Generation: The Larger Model solves problems from the training set to create a "Correct Reasoning" dataset.
Verifier Fine-tuning: The Smaller Model is fine-tuned using LoRA to predict a high score (1.0) for the Teacher's reasoning.
Solver Inference: The pipeline loads existing outputs from a "Solver" model (the model being verified).
Verification Scoring: The fine-tuned Smaller Model scores the Solver's reasoning based on its learned logic.
Performance Evaluation: The system compares the verifier's scores against the actual correctness of the solver's answers to calculate Accuracy, TP, FP, TN, and FN.
Semantic Similarity: The system evaluates how closely the Solver's reasoning mimics the Teacher's reasoning using cosine similarity embeddings.

ðŸ›  Contribution Guidelines
This is an InnerSource project. Please ensure all new features:
Follow PEP 8 coding standards.
Use the centralized Logger instead of print().
Maintain Modularity by keeping logic within the appropriate src/ files.